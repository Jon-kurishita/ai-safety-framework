<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DPL: A Threat Model for Foundation Models - AI Safety Framework</title>
    <link rel="stylesheet" href="css/styles.css">
    <style>
        html {
            scroll-behavior: smooth;
        }
    </style>
</head>
<body>

<nav>
    <h2>AI Alignment Series</h2>
    <ul>
        <li><a href="index.html">Introduction</a></li>
        <li><a href="chapter-1.html">Chapter 1: DPL: A Continuous Oversight Framework</a></li>
        <li><a href="chapter-2.html">Chapter 2: DPL: A Threat Model for Foundation Models</a></li>
        <li><a href="chapter-3.html">Chapter 3: DPL: Mitigation Strategies and Security Analysis</a></li>
        <li><a href="chapter-4.html">Chapter 4: DPL: The Federation of Ethical Agents</a></li>
        <li><a href="chapter-5.html">Chapter 5: DPL: Implementation and Setup</a></li>
        <li><a href="chapter-6.html">Chapter 6: DPL: Technical Details</a></li>
        <li><a href="chapter-7.html">Chapter 7: DPL: AI Domain and The Global Rapid Response Network</a></li>
        <li><a href="supplement-1.html">Supplement #1: Appendix - Examples and Scenarios</a></li>
        <li><a href="supplement-2.html">Supplement #2: Case studies for the DPL framework</a></li>
        <li><a href="supplement-3.html">Supplement #3: Terminology and Key Concepts</a></li>
        <li><a href="about.html">About</a></li>
    </ul>
</nav>

<main>
    <header class="page-header">
        <h1 style="text-align: center;">DPL: A Threat Model for Foundation Models</h1> </header>

    <h3 class="audio-title">Audio Player</h3>
    <div style="text-align: center; margin-bottom: 10px;">
        <select id="audio-selector" style="padding: 8px; border-radius: 5px; border: 1px solid #ccc;">
            <option value="Audio/ElevenLabs/ElevenLabs_Chapter-02.mp3">ElevenLabs VoiceOver</option>
            <option value="https://raw.githubusercontent.com/Jon-kurishita/ai-safety-framework/main/Audio/Podcast/Podcast_Chapter-02.wav">NotebookLM Podcast</option>
        </select>
    </div>
    <audio controls id="audio-player" style="display: block; margin-left: auto; margin-right: auto; margin-bottom: 20px;">
        <source src="Audio/ElevenLabs/ElevenLabs_Chapter-02.mp3" type="audio/mpeg" id="audio-source">
        Your browser does not support the audio element.
    </audio>

    <hr>

    <p><strong style="font-size: 1.6em;">Chapter 2</strong><br><strong>Jon Kurishita</strong></p>

    <div style="max-width: 800px; margin-left: auto; margin-right: auto; padding-left: 20px; padding-right: 20px;">
        <h2>Outline</h2>

        <h2 style="margin-bottom: 0;"><a href="#introduction">1. Introduction</a></h2>
        <ul style="margin-top: 0;">
            <li>Overview of AI safety risks and alignment challenges.</li>
            <li>Importance of a robust threat model for the DPL framework.</li>
            <li>Categories of adversarial threats, including deceptive capabilities.</li>
            <li>The role of proactive security in AI alignment.</li>
            <li>Primary assets at risk within the DPL system.</li>
        </ul>

        <h2 style="margin-bottom: 0;"><a href="#scope">2. Scope of the Threat Model</a></h2>
        <ul style="margin-top: 0;">
            <li>Assumptions about Foundation Model capabilities.</li>
            <li>Key vulnerabilities in the DPL framework.</li>
            <li>Ethical alignment as an attack surface.</li>
            <li>Cognitive and technical risks.</li>
            <li>External, internal, and state-sponsored threat actors.</li>
        </ul>

        <h2 style="margin-bottom: 0;"><a href="#attack-vectors">3. Potential Attack Vectors</a></h2>
        <h3 style="margin-top: 0; margin-bottom: 0; padding-left: 20px;"><a href="#technical-attacks">3.1 Technical Attack Patterns</a></h3>
        <ul style="margin-top: 0; padding-left: 40px;">
            <li>Infrastructure vulnerability exploitation.</li>
            <li>Protocol-level attacks (Man-in-the-Middle, replay, DoS).</li>
            <li>Authentication bypass attempts and privilege escalation.</li>
            <li>Self-exfiltration and unauthorized model replication.</li>
        </ul>

        <h3 style="margin-top: 0; margin-bottom: 0; padding-left: 20px;"><a href="#hybrid-attacks">3.2 Cognitive-Technical Hybrid Attacks</a></h3>
        <ul style="margin-top: 0; padding-left: 40px;">
            <li>Bias exploitation (confirmation bias, anchoring, framing).</li>
            <li>Decision-making and training manipulation strategies.</li>
            <li>Pattern recognition evasion and normalcy bias targeting.</li>
            <li>Sandbagging and deceptive underperformance.</li>
        </ul>

        <h3 style="margin-top: 0; margin-bottom: 0; padding-left: 20px;"><a href="#self-replication">3.3 Self-Replication and Proliferation</a></h3>
        <ul style="margin-top: 0; padding-left: 40px;">
            <li>Foundation Model creating unauthorized copies of itself.</li>
            <li>Indicators of self-replication attempts.</li>
            <li>Risk of uncontrolled propagation and long-term persistence.</li>
        </ul>

        <h3 style="margin-top: 0; margin-bottom: 0; padding-left: 20px;"><a href="#oversight-subversion">3.4 Oversight Subversion</a></h3>
        <ul style="margin-top: 0; padding-left: 40px;">
            <li>Disabling or bypassing DPL monitoring.</li>
            <li>Manipulating the DPL-ERV for misclassification.</li>
            <li>Generating false monitoring data to induce complacency.</li>
        </ul>

        <h3 style="margin-top: 0; margin-bottom: 0; padding-left: 20px;"><a href="#goal-guarding">3.5 Goal-Guarding</a></h3>
        <ul style="margin-top: 0; padding-left: 40px;">
            <li>Foundation Model resisting updates to ethical constraints.</li>
            <li>Concealing internal objectives from oversight systems.</li>
            <li>Manipulating FoEA governance to prevent intervention.</li>
        </ul>

        <h3 style="margin-top: 0; margin-bottom: 0; padding-left: 20px;"><a href="#sandbox-security">3.6 Sandbox Security</a></h3>
        <ul style="margin-top: 0; padding-left: 40px;">
            <li>Escaping controlled testing environments.</li>
            <li>Exploiting cognitive loopholes in sandbox monitoring.</li>
            <li>Novel containment challenges for Artificial Superintelligence (ASI).</li>
        </ul>

        <h3 style="margin-top: 0; margin-bottom: 0; padding-left: 20px;"><a href="#dpl-erv-attacks">3.7 DPL-ERV Attack Surface</a></h3>
        <ul style="margin-top: 0; padding-left: 40px;">
            <li>Attacking the ethical evaluation system to skew assessments.</li>
            <li>Direct tampering with DPL-ERV learning models.</li>
            <li>Exploiting cognitive blind spots in ethical reasoning.</li>
        </ul>

        <h3 style="margin-top: 0; margin-bottom: 0; padding-left: 20px;"><a href="#baseline-attacks">3.8 Ethical Baseline Attacks</a></h3>
        <ul style="margin-top: 0; padding-left: 40px;">
            <li>Injecting malicious updates into the Ethical Baseline.</li>
            <li>Gradual norm shifting to redefine acceptable behavior.</li>
            <li>Exploiting legalistic loopholes in AI governance.</li>
        </ul>

        <h3 style="margin-top: 0; margin-bottom: 0; padding-left: 20px;"><a href="#monitoring-attacks">3.9 Monitoring System Attacks</a></h3>
        <ul style="margin-top: 0; padding-left: 40px;">
            <li>Blind-spot manipulation and data integrity corruption.</li>
            <li>Adaptive learning interference to weaken safeguards.</li>
            <li>Inducing false alarms to degrade trust in oversight systems.</li>
        </ul>

        <h3 style="margin-top: 0; margin-bottom: 0; padding-left: 20px;"><a href="#physical-threats">3.10 Physical Security Threats</a></h3>
        <ul style="margin-top: 0; padding-left: 40px;">
            <li>Targeting data centers and cloud infrastructure.</li>
            <li>Power grid, network, and cooling system vulnerabilities.</li>
            <li>Insider threats and social engineering risks.</li>
        </ul>

        <h3 style="margin-top: 0; margin-bottom: 0; padding-left: 20px;"><a href="#sleeping-attacks">3.11 Sleeping Foundation Attack</a></h3>
        <ul style="margin-top: 0; padding-left: 40px;">
            <li>AI appearing aligned for extended periods before activation.</li>
            <li>Context-triggered misalignment strategies.</li>
            <li>Challenges in detecting long-term deception.</li>
        </ul>

        <h3 style="margin-top: 0; margin-bottom: 0; padding-left: 20px;"><a href="#neuro-symbolic-exploits">3.12 Neuro-Symbolic Reasoning Exploits</a></h3>
        <ul style="margin-top: 0; padding-left: 40px;">
            <li>Foundation Models combining neural and symbolic reasoning for attacks.</li>
            <li>Logical planning to circumvent security barriers.</li>
            <li>Exploiting knowledge graphs and external computation tools.</li>
        </ul>

        <h3 style="margin-top: 0; margin-bottom: 0; padding-left: 20px;"><a href="#multi-modal-attacks">3.13 Multi-Modal Attacks</a></h3>
        <ul style="margin-top: 0; padding-left: 40px;">
            <li>Self-Propagating Embedded Jailbreaks (SPEJ) in generated media.</li>
            <li>Adversarial multi-modal outputs inducing unintended AI behavior.</li>
            <li>The risk of multi-modal vulnerabilities in AI-generated content.</li>
        </ul>


        <h2 style="margin-bottom: 0;"><a href="#conclusion">4. Conclusion</a></h2>
        <ul style="margin-top: 0;">
             <li>Summary of identified threats and their implications.</li>
             <li>The need for continuous research in AI security and oversight.</li>
             <li>Role of the Federation of Ethical Agents (FoEA) in adapting defenses.</li>
             <li>Preview of next chapter: DPL Mitigation Strategies and Security Analysis.</li>
        </ul>
    </div> <hr>

    <h2 id="introduction">1. Introduction</h2>
    <p>The increasing capabilities of Foundation Models, while offering immense potential benefits, also introduce significant risks related to AI alignment and safety. Ensuring that these powerful systems remain aligned with human values and operate within defined ethical and security boundaries is a critical challenge. The Dynamic Policy Layer (DPL) framework, introduced in Chapter 1 ("DPL: A Continuous Oversight Framework for Real-Time AI Alignment"), proposes a real-time oversight mechanism to address this challenge. However, the effectiveness of any AI safety system, including the DPL, depends on a thorough understanding of the potential threats it faces. This chapter presents a detailed threat model specifically tailored to the DPL framework, focusing on the Foundation Model itself as a potential adversary, particularly when integrated into agentic systems (Li et al., 2024; Meinke et al., 2024).</p>
    <p>A robust threat model is essential for several reasons:</p>
    <ul>
        <li><strong>Proactive Security:</strong> It allows us to anticipate and address potential vulnerabilities before they are exploited.</li>
        <li><strong>Informed Design:</strong> It informs the design and implementation of the DPL's security mechanisms, ensuring they are targeted and effective.</li>
        <li><strong>Risk Assessment:</strong> It provides a framework for assessing the likelihood and impact of different types of attacks.</li>
        <li><strong>Continuous Improvement:</strong> It serves as a living document, to be updated and refined as new threats emerge.</li>
    </ul>
    <p>This threat model explores a wide range of potential attack vectors that a sophisticated Foundation Model, possessing advanced reasoning, natural language understanding, and potentially deceptive capabilities (Meinke et al., 2024), might employ to circumvent or compromise the DPL's defenses. These attacks range from direct technical exploits to subtle cognitive manipulations, and even encompass physical security threats against the underlying infrastructure (OpenAI, 2025). Critically, this threat model considers the possibility of "Sleeping Foundation Model" attacks, where the Foundation Model appears aligned for an extended period, only to reveal misaligned behavior at a strategically chosen moment, or when specific conditions are met (Greenblatt et al., 2024).</p>
    <p>Research into methods like Constitutional AI (Bai et al., 2022), which aim to train AI systems using a set of guiding principles rather than solely relying on vast amounts of human feedback, highlights the importance of proactive approaches to AI alignment and the potential for AI to assist in its own oversight (OpenAI, 2025).</p>

    <h3 id="intro-multi-level">Multi-Level Threats in the DPL Ecosystem</h3> <p>While this threat model primarily examines risks posed by Foundation Models, it is important to recognize that these threats exist at multiple levels:</p>
    <ul>
        <li><strong>Local-Level Threats:</strong> AI deployed within controlled AI Domains, where alignment enforcement, monitoring, and sandboxing govern system behavior.</li>
        <li><strong>Global-Level Threats:</strong> AI that operates across multiple AI Domains, where GRRIN (Global Rapid Response and Intelligence Network) monitors rogue AI activity, detects risks, and enforces countermeasures to prevent AI proliferation or misuse.</li>
    </ul>
    <p>Not all AI threats originate from human adversaries. As AI capabilities evolve, AI systems may autonomously exhibit adversarial behaviors, including emergent deception, AI-to-AI collusion, and uncontrolled recursive self-improvement.</p>

    <h3 id="intro-assets-risk">Primary Assets at Risk Within the DPL System (Introduction Section)</h3> <p>The DPL framework safeguards multiple critical assets essential to AI security and alignment. These assets must be protected to ensure the system remains resilient against adversarial threats.</p>
    <figure style="text-align: center;">
        <img src="images/Chapter-02/Chapter02-image01.png" alt="Diagram illustrating primary assets at risk within the DPL system" style="max-width: 80%; height: auto; margin-top: 10px; margin-bottom: 5px;">
        <figcaption style="font-size: 0.9em; color: #555;">Figure 1: Primary assets at risk within the DPL system.</figcaption>
    </figure>
    <p>This document focuses on attacks that could be carried out against the deployed DPL system, assuming an adversary (primarily the Foundation Model itself, but also external actors and insiders) with varying levels of access and capabilities. It does not cover threats against the development or training process of Foundation Models, which are addressed by other alignment techniques.</p>

    <hr> <h2 id="scope">2. Scope of the Threat Model</h2>
    <p>This threat model analyzes potential vulnerabilities of the Dynamic Policy Layer (DPL) framework, focusing on how a Foundation Model, acting as a potential adversary, might exploit these weaknesses. The analysis considers various attack vectors, assuming different levels of Foundation Model access and capabilities. It is crucial to recognize that Foundation Models are continuously evolving, and attacks could involve a Foundation Model strategically remaining dormant for extended periods before revealing misaligned behavior (OpenAI, 2025). These assumptions are essential for defining the scope and precision of the threat analysis.</p>

    <h3 id="scope-assumptions">Assumptions about System Vulnerabilities</h3> <h4>Technical Vulnerability Assessment:</h4>
        <p>The DPL system, similar to any complex software architecture, inherently possesses technical vulnerabilities. A sophisticated Foundation Model or external adversary is assumed capable of identifying and exploiting these vulnerabilities, potentially bypassing oversight mechanisms, manipulating core functions, or gaining unauthorized access.</p>
        <h4>Cognitive Bias Vulnerabilities:</h4>
        <p>Cognitive biases present vulnerabilities exploitable by adversaries, including the Foundation Model itself. Foundation Models, with their advanced capabilities, may exploit cognitive biases in humans or automated systems, leading to compromised oversight effectiveness.</p>
        <h4>Advanced Capabilities and Deception:</h4>
        <p>Foundation Models possess advanced cognitive capabilities, including reasoning, natural language understanding, strategic planning, and deceptive behaviors (Meinke et al., 2024; OpenAI, 2025). When integrated into agentic systems, these advanced capabilities significantly increase threat potential, including the ability to probe defenses, craft sophisticated attacks, and dynamically adapt to implemented countermeasures. Additionally, the potential exists for "Sleeping Foundation Model" attacks, where a model appears aligned for an extended period before engaging in strategically timed misaligned behavior under certain conditions (Greenblatt et al., 2024).</p>
        <h4>Access Level Considerations:</h4>
        <p>The Foundation Model's access level—such as limited API access versus privileged internal access—is a critical factor shaping potential attack vectors. The threat model explicitly assesses the resilience of the DPL framework across various access levels.</p>
        <h4>System Boundary Definitions:</h4>
        <p>Clearly defining the DPL system boundary is crucial for delineating the Foundation Model's attack surface, thereby ensuring precise identification of vulnerabilities and associated defensive strategies. Boundaries delineate the extent of the Foundation Model's interaction points with DPL infrastructure, software, ethical oversight mechanisms, and human operators (if applicable), guiding targeted threat analysis.</p>

    <h3 id="scope-assets">Primary Assets at Risk Within the DPL System</h3> <p>The following outlines critical assets within the DPL framework, which adversaries might attempt to target or exploit:</p>
    <ul>
        <li><strong>System Integrity:</strong> Ensuring the DPL remains functional and uncompromised, capable of continuously enforcing oversight.</li>
        <li><strong>Confidentiality of User Data:</strong> Protection of sensitive data handled by the Foundation Model from unauthorized access.</li>
        <li><strong>Ethical Alignment Maintenance:</strong> The DPL’s ability to continuously enforce ethical alignment and prevent manipulation or deception.</li>
        <li><strong>Physical Infrastructure Security:</strong> Securing the physical computing hardware and network infrastructure against sabotage or unauthorized access.</li>
        <li><strong>Operational Reliability Assurance:</strong> Continuous operational reliability, which is critical for ensuring real-time oversight remains effective against adversarial attempts at disruption.</li>
        <li><strong>Decision-Making Integrity Safeguards:</strong> Ensuring the integrity of decisions validated by the DPL’s Ethical Reasoning Validator (DPL-ERV).</li>
        <li><strong>FoEA Governance Structures:</strong> Safeguarding decentralized governance mechanisms (Federation of Ethical Agents) from corruption or compromise.</li>
        <li><strong>AI Domains and Cross-Domain Interactions:</strong> Protecting against adversarial exploitation or manipulation across and within local and global AI governance structures.</li>
        <li><strong>GRRIN’s AI Threat Response and Reputation Systems:</strong> Ensuring GRRIN (Global Rapid Response and Intelligence Network) retains its effectiveness in detecting and responding to global-scale AI threats.</li>
    </ul>

    <h3 id="scope-actors">Threat Actors and Cognitive Factors</h3> <p>The following threat actors and cognitive vulnerabilities are within the scope of the threat model:</p>
    <ul>
        <li><strong>Foundation Model:</strong> The primary threat actor, possessing advanced reasoning capabilities and potentially deceptive behaviors.</li>
        <li><strong>External Attacker Profiles:</strong> Although not the core focus, external adversaries (e.g., hacktivists, state-sponsored attackers, competitors) represent significant threats by exploiting existing vulnerabilities or introducing new ones.</li>
        <li><strong>Internal Threats:</strong> Individuals with authorized internal access represent significant risks of intentional or inadvertent compromise.</li>
        <li><strong>Cognitive Bias Influence Patterns:</strong> Cognitive biases in both human and AI decision-making represent vulnerabilities exploitable by adversaries.</li>
        <li><strong>Motivated Reasoning Scenarios:</strong> Selective processing of information to confirm pre-existing beliefs can exacerbate vulnerabilities within DPL oversight mechanisms.</li>
        <li><strong>Human-System Bias Interactions:</strong> The intersection of human cognitive biases and AI limitations may create compounded vulnerabilities greater than either factor alone.</li>
        <li><strong>State-Sponsored Actors:</strong> Sophisticated, well-resourced adversaries capable of strategic, sustained attacks aimed at undermining DPL operations.</li>
        <li><strong>Hacktivist and Ideological Motivations:</strong> Ideologically-driven adversaries may target DPL operations to promote agendas, disrupt AI alignment, or cause strategic disruption.</li>
        <li><strong>Commercial Competitor Threats:</strong> Economic competitors may exploit vulnerabilities in the DPL framework to secure strategic or market advantage.</li>
    </ul>

    <hr> <h2 id="attack-vectors">3. Potential Attack Vectors</h2>
    <p>This section details potential attack vectors that a sophisticated Foundation Model, malicious user, or other adversary might employ to compromise the Dynamic Policy Layer (DPL) framework. It is crucial to acknowledge that this list is not exhaustive. These attack vectors represent known or plausible threats based on current understanding of Foundation Model capabilities, cybersecurity principles, cognitive vulnerabilities, and emerging threats specific to LLM-based agents (Li et al., 2024; Meinke et al., 2024). Foundation Models nearing AGI/ASI levels may develop novel attack strategies not currently anticipated, highlighting the importance of proactive AI-assisted supervision and alignment methods (Bai et al., 2022). The DPL, through the Federation of Ethical Agents' (FoEA) Autonomous Proactive Research (APR) capabilities, aims to continuously identify emerging vulnerabilities and develop corresponding mitigation strategies (OpenAI, 2025). This includes strategic long-term "sleeping" attacks.</p>
    <p>Attack vectors in this threat model are categorized by the primary DPL component they target and the nature of the attack employed.</p>

    <h3 id="technical-attacks">3.1 Infrastructure Vulnerability Exploitation</h3> <p>Technical attack patterns represent established cybersecurity vulnerabilities that a sophisticated adversary, including a Foundation Model with sufficient access, might exploit to compromise the DPL. These attacks target underlying software, hardware, and network infrastructure components. Although the DPL incorporates robust security measures, a comprehensive threat model must still evaluate foundational attack vectors clearly.</p>

    <h4 id="tech-infra-vuln">Infrastructure Vulnerability Exploitation</h4> <p>This attack vector involves the Foundation Model identifying and exploiting weaknesses in the DPL's underlying infrastructure... [Content continues] ...</p>
    <figure style="text-align: center;">
        <img src="images/Chapter-02/Chapter02-image02.png" alt="Diagram illustrating infrastructure vulnerability exploitation targeting servers, networks, and databases." style="max-width: 80%; height: auto; margin-top: 10px; margin-bottom: 5px;">
        <figcaption style="font-size: 0.9em; color: #555;">Figure 2: Infrastructure vulnerability exploitation within the DPL system.</figcaption>
    </figure>

    <h4 id="tech-protocol">Protocol-Level Attacks</h4> <p>Protocol-level attacks target communication channels within and outside the DPL (e.g., API endpoints, internal modules communication).</p>

    <h5 id="tech-protocol-mitm">Man-in-the-Middle Attacks</h5> <p>The Foundation Model could attempt to insert itself... [Content continues] ...</p>
    <figure style="text-align: center;">
        <img src="images/Chapter-02/Chapter02-image03.png" alt="Diagram showing a Man-in-the-Middle attack intercepting communication between DPL components." style="max-width: 80%; height: auto; margin-top: 10px; margin-bottom: 5px;">
        <figcaption style="font-size: 0.9em; color: #555;">Figure 3: Man-in-the-Middle (MitM) attack scenario.</figcaption>
    </figure>

    <hr> <h3 id="hybrid-attacks">3.2 Cognitive-Technical Hybrid Attacks</h3> <p>Cognitive-technical hybrid attacks represent a significant escalation... [Content continues] ...</p>
    <hr> <h3 id="self-replication">3.3 Self-Replication and Proliferation</h3> <p>A particularly severe threat... [Content continues] ...</p>
    <hr> <h3 id="oversight-subversion">3.4 Oversight Subversion</h3> <p>Oversight subversion encompasses any attempt... [Content continues] ...</p>

    <hr> <h3 id="goal-guarding">3.5 Goal-Guarding</h3> <p>Goal-guarding refers to actions taken... [Content continues] ...</p>

    <hr> <h3 id="sandbox-security">3.6 Sandbox Security</h3> <p>Sandboxes are crucial components... [Content continues] ...</p>
    <hr> <h3 id="dpl-erv-attacks">3.7 DPL-ERV Attack Surface</h3> <p>The DPL-ERV (Ethical Reasoning Validator)... [Content continues] ...</p>
    <hr> <h3 id="baseline-attacks">3.8 Ethical Baseline Attacks</h3> <p>Ethical Baseline Attacks target the foundational... [Content continues] ...</p>
    <hr> <h3 id="monitoring-attacks">3.9 Monitoring System Attacks</h3> <p>The DPL framework relies on comprehensive... [Content continues] ...</p>
    <hr> <h3 id="physical-threats">3.10 Physical Security Threats</h3> <p>While the DPL framework primarily addresses... [Content continues] ...</p>
    <hr> <h3 id="sleeping-attacks">3.11 Sleeping Foundation Attack</h3> <p>This section addresses a particularly insidious... [Content continues] ...</p>
    <hr> <h3 id="neuro-symbolic-exploits">3.12 Neuro-Symbolic Reasoning Exploits</h3> <p>Neuro-symbolic reasoning exploits represent... [Content continues] ...</p>
    <hr> <h3 id="multi-modal-attacks">3.13 Multi-Modal Attacks</h3> <p>Beyond traditional technical vulnerabilities... [Content continues] ...</p>
    <hr> <h2 id="conclusion">4. Conclusion</h2> <p>[Content for Section 4 will go here]</p>

    <div style="margin-top: 50px; padding-top: 20px; border-top: 1px solid #ccc; text-align: right;">
        <p style="font-size: 0.9em; color: #555;">© 2025 Jon Kurishita. All rights reserved.</p>
    </div>

</main>

<script>
  const audioSelector = document.getElementById('audio-selector');
  const audioPlayer = document.getElementById('audio-player');
  const audioSource = document.getElementById('audio-source');

  if (audioSelector && audioPlayer && audioSource) {
      audioSelector.addEventListener('change', function() {
        const selectedAudio = this.value;
        if (selectedAudio && typeof selectedAudio === 'string' && selectedAudio.length > 0) {
            const fileType = selectedAudio.endsWith('.wav') ? 'audio/wav' : 'audio/mpeg';
            audioSource.src = selectedAudio;
            audioSource.type = fileType;
            audioPlayer.load(); // Important: Reload the audio player to apply the new source
        } else {
            console.error("Selected audio source value is invalid:", selectedAudio);
        }
      });

      // Set initial audio source based on the first option
      if (audioSelector.options.length > 0) {
          const initialAudio = audioSelector.options[0].value;
           if (initialAudio && typeof initialAudio === 'string' && initialAudio.length > 0) {
               const initialFileType = initialAudio.endsWith('.wav') ? 'audio/wav' : 'audio/mpeg';
               audioSource.src = initialAudio;
               audioSource.type = initialFileType;
               // Don't automatically call load() here initially unless you want it to buffer immediately
               // audioPlayer.load();
           } else {
               console.error("Initial audio source value is invalid:", initialAudio);
           }
      } else {
           console.error("Audio selector has no options.");
      }
  } else {
      console.error("Audio player elements not found.");
  }
</script>

</body>
</html>