<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>References - AI Safety Framework</title>
    <link rel="stylesheet" href="css/styles.css">
    <style>
        html {
            scroll-behavior: smooth;
        }
        .content-container {
             max-width: 800px;
             margin-left: auto;
             margin-right: auto;
             padding-left: 20px;
             padding-right: 20px;
        }
        .references-list ol {
            list-style-type: none; /* Remove default numbering */
            padding-left: 0; /* Remove default padding */
        }
        .references-list li {
            margin-bottom: 1em; /* Add space between references */
        }
        .footer-style {
             margin-top: 50px;
             padding-top: 20px;
             border-top: 1px solid #ccc;
             text-align: right;
        }
        .footer-text-style {
            font-size: 0.9em;
            color: #555;
        }
    </style>
</head>
<body>

<nav>
    <h2>AI Alignment Series</h2>
    <ul>
        <li><a href="index.html">Introduction</a></li>
        <li><a href="chapter-1.html">Chapter 1: DPL: A Continuous Oversight Framework</a></li>
        <li><a href="chapter-2.html">Chapter 2: DPL: A Threat Model for Foundation Models</a></li>
        <li><a href="chapter-3.html">Chapter 3: DPL: Mitigation Strategies and Security Analysis</a></li>
        <li><a href="chapter-4.html">Chapter 4: DPL: The Federation of Ethical Agents</a></li>
        <li><a href="chapter-5.html">Chapter 5: DPL: Implementation and Setup</a></li>
        <li><a href="chapter-6.html">Chapter 6: DPL: Technical Details</a></li>
        <li><a href="chapter-7.html">Chapter 7: DPL: AI Domain and The Global Rapid Response Network</a></li>
        <li><a href="supplement-1.html">Supplement #1: Appendix - Examples and Scenarios</a></li>
        <li><a href="supplement-2.html">Supplement #2: Case studies for the DPL framework</a></li>
        <li><a href="supplement-3.html">Supplement #3: Terminology and Key Concepts</a></li>
        <li><a href="references.html">References</a></li>
        <li><a href="downloads.html">Downloads (PDF)</a></li>
        <li><a href="about.html">About</a></li>
    </ul>
</nav>

<main>
    <header class="page-header">
        <h1>References</h1>
    </header>

    <hr>

    <div class="content-container">
        <div class="references-list">
            <ol>
                <li>[1] Greenblatt, R., et al. (2024). Alignment faking in large language models. arXiv preprint arXiv:2412.14093. Retrieved from <a href="https://arxiv.org/abs/2412.14093" target="_blank">https://arxiv.org/abs/2412.14093</a></li>
                <li>[2] Meinke, A., Schoen, B., Scheurer, J., Balesni, M., Shah, R., & Hobbhahn, M. (2024). Frontier models are capable of in-context scheming. arXiv preprint arXiv:2412.04984. <a href="https://doi.org/10.48550/arXiv.2412.04984" target="_blank">https://doi.org/10.48550/arXiv.2412.04984</a></li>
                <li>[3] OpenAI. (2024). OpenAI o1 System Card. <a href="https://arxiv.org/abs/2412.16720" target="_blank">https://arxiv.org/abs/2412.16720</a></li>
                <li>[4] OpenAI. (2025). OpenAI o3-mini System Card. <a href="https://cdn.openai.com/o3-mini-system-card.pdf" target="_blank">https://cdn.openai.com/o3-mini-system-card.pdf</a></li>
                <li>[5] Alignment Science Team. (2025). Recommendations for technical AI safety research directions. Anthropic Alignment Blog. <a href="https://alignment.anthropic.com/2025/recommended-directions" target="_blank">https://alignment.anthropic.com/2025/recommended-directions</a></li>
                <li>[6] Bai, Y., et al. (2022). Constitutional AI: Harmlessness from AI Feedback. arXiv preprint arXiv:2212.08073. Retrieved from <a href="https://arxiv.org/abs/2212.08073" target="_blank">https://arxiv.org/abs/2212.08073</a></li>
                <li>[7] Hubinger, E., et al. (2024). Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. arXiv preprint arXiv:2401.05566. <a href="https://arxiv.org/pdf/2401.05566" target="_blank">https://arxiv.org/pdf/2401.05566</a></li>
                <li>[8] Geiping, J., et al. (2025). Scaling up test-time compute with latent reasoning: A recurrent depth approach. arXiv preprint arXiv:2502.05171. Retrieved from <a href="http://arxiv.org/abs/2502.05171" target="_blank">http://arxiv.org/abs/2502.05171</a></li>
                <li>[9] Mitchell, M., Ghosh, A., Luccioni, A. S., & Pistilli, G. (2025). Fully autonomous AI agents should not be developed. arXiv preprint arXiv:2502.02649. Retrieved from <a href="https://arxiv.org/abs/2502.02649" target="_blank">https://arxiv.org/abs/2502.02649</a>.</li>
                <li>[10] Pan, X., Dai, J., Fan, Y., & Yang, M. (2024). Frontier AI systems have surpassed the self-replicating red line. arXiv preprint arXiv:2412.12140. <a href="https://doi.org/10.48550/arXiv.2412.12140" target="_blank">https://doi.org/10.48550/arXiv.2412.12140</a></li>
                <li>[11] OpenAI et al. (2025). Competitive Programming with Large Reasoning Models. arXiv. <a href="https://doi.org/10.48550/arXiv.2502.06807" target="_blank">https://doi.org/10.48550/arXiv.2502.06807</a></li>
                <li>[12] Li, A., Zhou, Y., Raghuram, V. C., Goldstein, T., & Goldblum, M. (2025). Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous Attacks. arXiv:2502.08586. <a href="https://arxiv.org/abs/2502.08586" target="_blank">https://arxiv.org/abs/2502.08586</a></li>
                <li>[13] Leahy, C., Alfour, G., Scammell, C., Miotti, A., & Shimi, A. (2024). The Compendium (V1.3.1). [Living document]. Retrieved from <a href="https://pdf.thecompendium.ai/the_compendium.pdf" target="_blank">https://pdf.thecompendium.ai/the_compendium.pdf</a></li>
                <li>[14] Hausenloy, J., Miotti, A., & Dennis, C. (2023). Multinational AGI Consortium (MAGIC): A Proposal for International Coordination on AI. arXiv:2310.09217. <a href="https://arxiv.org/abs/2310.09217" target="_blank">https://arxiv.org/abs/2310.09217</a></li>
                <li>[15] Aasen, D., Aghaee, M., Alam, Z., Andrzejczuk, M., Antipov, A., Astafev, M., ... Mei, A. R. (2025). Roadmap to fault tolerant quantum computation using topological qubit arrays. arXiv. <a href="https://doi.org/10.48550/arXiv.2502.12252" target="_blank">https://doi.org/10.48550/arXiv.2502.12252</a></li>
                <li>[16] Sarkar, B., Xia, W., Liu, C. K., & Sadigh, D. (2025). Training language models for social deduction with multi-agent reinforcement learning. In Proceedings of the 24th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2025), Detroit, Michigan, USA. IFAAMAS. <a href="https://arxiv.org/abs/2502.06060" target="_blank">https://arxiv.org/abs/2502.06060</a></li>
                <li>[17] Anthropic. (2025, February 24). Claude 3.7 Sonnet System Card. Anthropic. <a href="https://www.anthropic.com/claude-3-7-sonnet-system-card" target="_blank">https://www.anthropic.com/claude-3-7-sonnet-system-card</a></li>
                <li>[18] TIME. (2024, December 18). Exclusive: New Research Shows AI Strategically Lying. Retrieved from <a href="https://time.com/7202784/ai-research-strategic-lying" target="_blank">https://time.com/7202784/ai-research-strategic-lying</a></li>
                <li>[19] Vijayan, J. (2023, December 5). LLMs Open to Manipulation Using Doctored Images, Audio. Dark Reading. Retrieved from <a href="https://www.darkreading.com/vulnerabilities-threats/llms-open-manipulation-using-doctored-images-audio" target="_blank">https://www.darkreading.com/vulnerabilities-threats/llms-open-manipulation-using-doctored-images-audio</a></li>
            </ol>
        </div>
    </div>

    <footer class="footer-style">
        <p class="footer-text-style">Â© 2025 Jon Kurishita. All rights reserved.</p>
    </footer>

</main>

</body>
</html>