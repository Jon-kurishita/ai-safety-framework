<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 1: DPL: A Continuous Oversight Framework - AI Safety Framework</title>
    <link rel="stylesheet" href="css/styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
</head>
<body>

    <nav>
        <h2>AI Alignment Series</h2>
        <ul>
            <li><a href="index.html">Introduction</a></li>
            <li><a href="chapter-1.html">Chapter 1: DPL: A Continuous Oversight Framework</a></li>
            <li><a href="chapter-2.html">Chapter 2: DPL: A Threat Model for Foundation Models</a></li>
            <li><a href="chapter-3.html">Chapter 3: DPL: Mitigation Strategies and Security Analysis</a></li>
            <li><a href="chapter-4.html">Chapter 4: DPL: The Federation of Ethical Agents</a></li>
            <li><a href="chapter-5.html">Chapter 5: DPL: Implementation and Setup</a></li>
            <li><a href="chapter-6.html">Chapter 6: DPL: Technical Details</a></li>
            <li><a href="chapter-7.html">Chapter 7: DPL: AI Domain and The Global Rapid Response Network</a></li>
            <li><a href="supplement-1.html">Supplement #1: Appendix - Examples and Scenarios</a></li>
            <li><a href="supplement-2.html">Supplement #2: Case studies for the DPL framework</a></li>
            <li><a href="supplement-3.html">Supplement #3: Terminology and Key Concepts</a></li>
            <li><a href="about.html">About</a></li>
        </ul>
    </nav>

    <main>
        <h1 style="text-align: center; margin-bottom: 0;">DPL: A Continuous Oversight Framework</h1>
        <p style="text-align: center; font-size: 1.2em; margin-top: 5px; margin-bottom: 15px; color: #555;">Chapter 1</p>
        <p style="text-align: center;"><em>Jon Kurishita</em></p>

        <hr>

        <div class="audio-section" style="text-align: center; margin-top: 20px; margin-bottom: 30px;">
            <h3 style="color: darkblue; margin-bottom: 10px; text-align: center;">Audio Player</h3> {/* <-- Changed Title */}
            {/* === IMPORTANT: Verify audio file URLs below === */}
            {/* === Consider converting WAV to MP3 for better compatibility === */}
            <select id="audio-selector" style="padding: 8px; border-radius: 5px; border: 1px solid #ccc; margin-bottom: 10px;">
                <option value="https://raw.githubusercontent.com/Jon-kurishita/ai-safety-framework/main/Audio/ElevenLabs/ElevenLabs_Chapter-01.mp3">ElevenLabs VoiceOver</option>
                <option value="https://raw.githubusercontent.com/Jon-kurishita/ai-safety-framework/main/Audio/Podcast/Podcast_Chapter-01.wav">NotebookLM Podcast</option>
            </select>
            <audio id="audio-player" controls style="width: 80%; max-width: 600px; display: block; margin-left: auto; margin-right: auto;">
                <source id="audio-source" src="https://raw.githubusercontent.com/Jon-kurishita/ai-safety-framework/main/Audio/ElevenLabs/ElevenLabs_Chapter-01.mp3" type="audio/mpeg">
                Your browser does not support the audio element.
            </audio>
        </div>

        <hr>

        <h2 style="text-align: center; font-size: 2em;">Outline</h2> {/* <-- Increased font size */}
        <ul>
            <li><a href="#introduction"><strong>Introduction</strong></a>
                <ul>
                    <li>Challenges of AI alignment</li>
                    <li>The role of the Dynamic Policy Layer (DPL)</li>
                    <li>Core principles of the DPL</li>
                    <li>Relationship with the Federation of Ethical Agents (FoEA)</li>
                </ul>
            </li>
            <li><a href="#core-dpl"><strong>Core DPL Concepts and Architecture</strong></a>
                <ul>
                    <li>Design principles</li>
                    <li>Modularity and adaptability</li>
                    <li>Key components</li>
                    <li>Data flow and workflow</li>
                </ul>
            </li>
            <li><a href="#dpl-erv"><strong>The DPL-ERV: Ethical Reasoning and Validation</strong></a>
                <ul>
                    <li>Role of the DPL-ERV</li>
                    <li>Integration with the Ethical Baseline</li>
                    <li>Value Modules for ethical evaluations</li>
                    <li>Transparency and explainability</li>
                    <li>Reinforcement Learning from Ethical Feedback (RLEF)</li>
                </ul>
            </li>
             <li><a href="#mitigation"><strong>High-Level Overview of Mitigation Strategies</strong></a>
                <ul>
                    <li>Technical controls</li>
                    <li>Cognitive bias countermeasures</li>
                    <li>Ethical reasoning and validation</li>
                    <li>Real-time monitoring and anomaly detection</li>
                    <li>Tiered intervention system</li>
                    <li>False positive reduction</li>
                    <li>Autonomous Proactive Research (APR)</li>
                </ul>
            </li>
            <li><a href="#foea"><strong>The Federation of Ethical Agents (FoEA): Overview</strong></a>
                <ul>
                    <li>Core functions of the FoEA</li>
                    <li>Governance of the DPL-ERV</li>
                    <li>Ethical Baseline maintenance</li>
                    <li>Autonomous Proactive Research (APR)</li>
                    <li>Security oversight</li>
                </ul>
            </li>
            <li><a href="#threat-model"><strong>Threat Model: Overview</strong></a>
                 <ul>
                    <li>Alignment faking</li>
                    <li>In-context scheming</li>
                    <li>Dynamic misalignment</li>
                    <li>Oversight subversion</li>
                    <li>Emergent communication</li>
                    <li>Ethical baseline attacks</li>
                </ul>
            </li>
            <li><a href="#future-research"><strong>Future Research Directions</strong></a>
                <ul>
                    <li>Autonomous threat discovery</li>
                    <li>Emergent communication risks</li>
                    <li>Scalable oversight for AGI/ASI</li>
                    <li>Formal verification</li>
                    <li>Quantum-resistant cryptography</li>
                    <li>Digital Sanctuaries for AI development</li>
                </ul>
            </li>
            <li><a href="#limitations"><strong>Limitations</strong></a>
                <ul>
                    <li>Single-model focus</li>
                    <li>Dependence on detectability</li>
                    <li>Computational overhead</li>
                    <li>FoEA vulnerabilities</li>
                    <li>Ethical Baseline completeness</li>
                </ul>
            </li>
            <li><a href="#conclusion"><strong>Conclusion</strong></a>
                <ul>
                    <li>Contributions of the DPL framework</li>
                    <li>Long-term vision and challenges</li>
                    <li>Future research and adaptation</li>
                </ul>
            </li>
            <li><a href="#references"><strong>References</strong></a>
                <ul>
                    <li>List of cited works</li>
                </ul>
            </li>
        </ul>

        <hr style="margin-top: 30px; margin-bottom: 30px;">

        <h2 id="introduction">Introduction</h2>
        <p>...</p>
        <p>...</p>
        <p>...</p>
        <p>...</p>
        <p>...</p>

        <h2 id="core-dpl">1. Core DPL Concepts and Architecture</h2>
        <p>...</p>
        <h3>Design Principles</h3>
        <p>...</p>
        <ul>...</ul>
        <h3>Key Components</h3>
        <p>...</p>
        <ul>...</ul>
        <h3>Data Flow and Workflow (High-Level)</h3>
        <div style="text-align: center;">
             <img src="images/Chapter01-image01.png" alt="Diagram illustrating the DPL Data Flow and Workflow" style="max-width: 100%; height: auto; margin: 20px 0;">
        </div>
        <p>...</p>
        <ol>...</ol>
        <p>...</p>

        <h2 id="dpl-erv">2. The DPL-ERV: Ethical Reasoning and Validation</h2>
        <p>...</p>
        <div style="text-align: center;">
             <img src="images/Chapter01-image02.png" alt="Diagram illustrating the DPL-ERV component" style="max-width: 100%; height: auto; margin: 20px 0;">
        </div>
        <h3>Role of the DPL-ERV</h3>
        <p>...</p>
        <ul>...</ul>
        <h3>Ethical Baseline Integration</h3>
        <p>...</p>
        <ul>...</ul>
        <h3>High-Level Description of Value Modules</h3>
        <p>...</p>
        <ul>...</ul>
        <p>...</p>
        <h3>Transparency and Explainability (Ethical Chain-of-Thought)</h3>
        <p>...</p>
        <ul>...</ul>
        <h3>RLEF</h3>
        <p>...</p>

        <h2 id="mitigation">3. High-Level Overview of Mitigation Strategies</h2>
        <p>...</p>
        <p>...</p>
        <ul>...</ul>
        <h3>Key Principles:</h3>
        <ul>...</ul>
        <p>...</p>

        <h2 id="foea">4. The Federation of Ethical Agents (FoEA): Overview</h2>
        <p>...</p>
        <p>...</p>
        <ul>...</ul>
        <p>...</p>

        <h2 id="threat-model">5. Threat Model: Overview</h2>
        <p>...</p>
        <p>...</p>
        <ul>...</ul>
        <p>...</p>

        <h2 id="future-research">6. Future Research Directions</h2>
        <p>...</p>
        <h3>Autonomous Threat Discovery and Mitigation:</h3>
        <p>...</p>
        <ul>...</ul>
        <h3>Emergent Communication Risks:</h3>
        <p>...</p>
        <ul>...</ul>
        <h3>Scalable Oversight for AGI/ASI:</h3>
        <p>...</p>
        <ul>...</ul>
        <h3>"Move 37" Analogy and Beneficial Emergent Behavior:</h3>
        <p>...</p>
        <ul>...</ul>
        <h3>Refinement of RLEF:</h3>
        <p>...</p>
        <ul>...</ul>
        <h3>Formal Verification (where feasible):</h3>
        <p>...</p>
        <ul>...</ul>
        <h3>"Graduation" Criteria and Phased Reduction of Controls:</h3>
        <p>...</p>
        <ul>...</ul>
        <h3>Preparing for the Quantum Threat:</h3>
        <p>...</p>
        <ul>...</ul>
        <p>...</p>
        <h3>Digital Sanctuaries for AI Exploration, Ethical Development, and Well-being</h3>
        <p>...</p>
        <h4>Key Concepts:</h4>
        <ul>...</ul>
        <h4>Potential Benefits:</h4>
        <ul>...</ul>
        <h4>Challenges and Research Directions:</h4>
        <p>...</p>
        <ul>...</ul>
        <p>...</p>
        <div style="text-align: center;">
            <img src="images/Chapter01-image03.png" alt="Diagram illustrating the Digital Sanctuary concept" style="max-width: 100%; height: auto; margin: 20px 0;">
        </div>

        <h2 id="limitations">7. Limitations</h2>
        <p>...</p>
        <ul>...</ul>
        <p>...</p>

        <h2 id="conclusion">8. Conclusion</h2>
        <p>...</p>
        <p>...</p>
        <ul>...</ul>
        <p>...</p>
        <p>...</p>
        <p>...</p>

        <h2 id="references">References</h2>
        <div class="references-list">
             {/* --- Added links to references --- */}
            <p>[1] Greenblatt, R., et al. (2024). Alignment faking in large language models. arXiv preprint arXiv:2412.14093. Retrieved from <a href="https://arxiv.org/abs/2412.14093" target="_blank">https://arxiv.org/abs/2412.14093</a></p>
            <p>[2] Meinke, A., Schoen, B., Scheurer, J., Balesni, M., Shah, R., & Hobbhahn, M. (2024). Frontier models are capable of in-context scheming. arXiv preprint arXiv:2412.04984. <a href="https://doi.org/10.48550/arXiv.2412.04984" target="_blank">https://doi.org/10.48550/arXiv.2412.04984</a></p>
            <p>[3] OpenAI. (2024). OpenAI o1 System Card. <a href="https://arxiv.org/abs/2412.16720" target="_blank">https://arxiv.org/abs/2412.16720</a></p>
            <p>[4] OpenAI. (2025). OpenAI o3-mini System Card. <a href="https://cdn.openai.com/o3-mini-system-card.pdf" target="_blank">https://cdn.openai.com/o3-mini-system-card.pdf</a></p>
            <p>[5] Alignment Science Team. (2025). Recommendations for technical AI safety research directions. Anthropic Alignment Blog. <a href="https://alignment.anthropic.com/2025/recommended-directions" target="_blank">https://alignment.anthropic.com/2025/recommended-directions</a></p>
            <p>[6] Bai, Y., et al. (2022). Constitutional AI: Harmlessness from AI Feedback. arXiv preprint arXiv:2212.08073. Retrieved from <a href="https://arxiv.org/abs/2212.08073" target="_blank">https://arxiv.org/abs/2212.08073</a></p>
            <p>[7] Hubinger, E., et al. (2024). Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. arXiv preprint arXiv:2401.05566. <a href="https://arxiv.org/pdf/2401.05566" target="_blank">https://arxiv.org/pdf/2401.05566</a></p>
            <p>[8] Geiping, J., et al. (2025). Scaling up test-time compute with latent reasoning: A recurrent depth approach. arXiv preprint arXiv:2502.05171. Retrieved from <a href="http://arxiv.org/abs/2502.05171" target="_blank">http://arxiv.org/abs/2502.05171</a></p>
            <p>[9] Mitchell, M., Ghosh, A., Luccioni, A. S., & Pistilli, G. (2025). Fully autonomous AI agents should not be developed. arXiv preprint arXiv:2502.02649. Retrieved from <a href="https://arxiv.org/abs/2502.02649" target="_blank">https://arxiv.org/abs/2502.02649</a></p>
            <p>[10] Pan, X., Dai, J., Fan, Y., & Yang, M. (2024). Frontier AI systems have surpassed the self-replicating red line. arXiv preprint arXiv:2412.12140. <a href="https://doi.org/10.48550/arXiv.2412.12140" target="_blank">https://doi.org/10.48550/arXiv.2412.12140</a></p>
            <p>[11] OpenAI et al. (2025). Competitive Programming with Large Reasoning Models. arXiv. <a href="https://doi.org/10.48550/arXiv.2502.06807" target="_blank">https://doi.org/10.48550/arXiv.2502.06807</a></p>
            <p>[12] Li, A., Zhou, Y., Raghuram, V. C., Goldstein, T., & Goldblum, M. (2025). Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous Attacks. arXiv:2502.08586. <a href="https://arxiv.org/abs/2502.08586" target="_blank">https://arxiv.org/abs/2502.08586</a></p>
            <p>[13] Leahy, C., Alfour, G., Scammell, C., Miotti, A., & Shimi, A. (2024). The Compendium (V1.3.1). [Living document]. Retrieved from <a href="https://pdf.thecompendium.ai/the_compendium.pdf" target="_blank">https://pdf.thecompendium.ai/the_compendium.pdf</a></p>
            <p>[14] Hausenloy, J., Miotti, A., & Dennis, C. (2023). Multinational AGI Consortium (MAGIC): A Proposal for International Coordination on AI. arXiv:2310.09217. <a href="https://arxiv.org/abs/2310.09217" target="_blank">https://arxiv.org/abs/2310.09217</a></p>
            <p>[15] Aasen, D., Aghaee, M., Alam, Z., Andrzejczuk, M., Antipov, A., Astafev, M., ... Mei, A. R. (2025). Roadmap to fault tolerant quantum computation using topological qubit arrays. arXiv. <a href="https://doi.org/10.48550/arXiv.2502.12252" target="_blank">https://doi.org/10.48550/arXiv.2502.12252</a></p>
            <p>[16] Sarkar, B., Xia, W., Liu, C. K., & Sadigh, D. (2025). Training language models for social deduction with multi-agent reinforcement learning. In Proceedings of the 24th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2025), Detroit, Michigan, USA. IFAAMAS. <a href="https://arxiv.org/abs/2502.06060" target="_blank">https://arxiv.org/abs/2502.06060</a></p>
            <p>[17] Anthropic. (2025, February 24). Claude 3.7 Sonnet System Card. Anthropic. <a href="https://www.anthropic.com/claude-3-7-sonnet-system-card" target="_blank">https://www.anthropic.com/claude-3-7-sonnet-system-card</a></p>
        </div>

        <div style="margin-top: 50px; padding-top: 20px; border-top: 1px solid #ccc; text-align: right;">
            <p style="font-size: 0.9em; color: #555;">© 2025 Jon Kurishita. All rights reserved.</p>
        </div>

    </main> <script src="js/script.js"></script>

</body>
</html>