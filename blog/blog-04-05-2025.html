Hi Everyone,
Artificial intelligence is rapidly transforming our world, but with great power comes great responsibility. How can we ensure that AI systems remain aligned with human values? How do we prevent potential risks such as misalignment, unintended behaviors, or even existential threats?

I’m Jon Kurishita, a retired engineer turned AI safety researcher, and I created this blog to explore these critical questions. Through my AI Safety Alignment Series, I aim to break down the complexities of AI alignment, explore cutting-edge research, and propose ideas for ensuring AI remains beneficial to humanity.

This series is designed for researchers, engineers, policymakers, and anyone interested in AI safety, alignment theory, and the future of artificial intelligence.


This series delves into the fundamental challenges of AI alignment and explores possible solutions. Each chapter tackles a key aspect of AI safety, covering topics such as:
✅ Understanding AI alignment and its importance
✅ Risks of misaligned AI and real-world consequences
✅ Technical and philosophical challenges in AI safety
✅ Proposals and frameworks for AI governance
✅ Exploring interpretability, corrigibility, and control mechanisms