<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Blog Posts - AI Alignment Series</title> 
    <link rel="stylesheet" href="css/styles.css">
</head>
<body> 
<nav>
    <h2>AI Alignment Series</h2>
    <ul>
        <li><a href="index.html">Introduction</a></li>
        <li><a href="chapter-1.html">Chapter 1: DPL: A Continuous Oversight Framework</a></li>
        <li><a href="chapter-2.html">Chapter 2: DPL: A Threat Model for Foundation Models</a></li>
        <li><a href="chapter-3.html">Chapter 3: DPL: Mitigation Strategies and Security Analysis</a></li>
        <li><a href="chapter-4.html">Chapter 4: DPL: The Federation of Ethical Agents</a></li>
        <li><a href="chapter-5.html">Chapter 5: DPL: Implementation and Setup</a></li>
        <li><a href="chapter-6.html">Chapter 6: DPL: Technical Details</a></li>
        <li><a href="chapter-7.html">Chapter 7: DPL: AI Domain and The Global Rapid Response Network</a></li>
        <li><a href="supplement-1.html">Supplement #1: Appendix - Examples and Scenarios</a></li>
        <li><a href="supplement-2.html">Supplement #2: Case studies for the DPL framework</a></li>
        <li><a href="supplement-3.html">Supplement #3: Terminology and Key Concepts</a></li>
        <li><a href="references.html">References</a></li>
        <li><a href="downloads.html">Downloads (PDF)</a></li>
        <li><a href="about.html">About</a></li>
        <li><a href="blog.html">Blog Posts</a></li> 
    </ul>
    <button id="theme-toggle-button" aria-label="Toggle dark mode">
        Toggle Theme
    </button>
</nav>

<main>
    <header class="page-header">
        
        <h1>Blog Posts</h1>
    </header>

    <div class="content-container" id="blog-post-list">
        
        <article class="blog-entry">
            <h2><a href="blog/blog-04-07-2025.html">Navigating the Precipice: Steering AI Alignment with Urgency and Caution</a></h2>
            <p class="post-meta"><em>Posted on: <time datetime="2025-04-07">April 7, 2025</time></em></p>
            <p>This post argues for urgently yet cautiously building AI safety frameworks, using a mountain descent metaphor to illustrate the risks of both inaction and reckless speed.</p>
            <hr> 
        </article>

        <article class="blog-entry">
            <h2><a href="blog/blog-04-05-2025.html">Welcome to the AI Safety Alignment Series Blog</a></h2>
            <p class="post-meta"><em>Posted on: <time datetime="2025-04-07">April 7, 2025</time></em></p>
            <p>AI safety researcher Jon Kurishita introduces this blog series exploring the vital challenges and potential solutions for aligning powerful AI with human values.</p>
            <hr> 
        </article>
        

    </div>



    <footer class="footer-style">
        <p class="footer-text-style">Â© 2025 Jon Kurishita. All rights reserved.</p>
    </footer>

</main>

<script src="js/theme-toggle.js" defer></script> 

</body>
</html>
